{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxaynu5lAYSaA3eYvNGR4j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Madhan-sukumar/CNN/blob/main/Image%20Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## About Dataset:\n",
        "- CIFAR-10 Photo Classification Dataset\n",
        "- CIFAR is an acronym that stands for the Canadian Institute For Advanced Research and the CIFAR-10 dataset was developed along with the CIFAR-100 dataset by researchers at the CIFAR institute.\n",
        "\n",
        "- The dataset is comprised of 60,000 32×32 pixel color photographs of objects from 10 classes, such as frogs, birds, cats, ships, etc. The class labels and their standard associated integer values are listed below.\n",
        "\n",
        "- 0: airplane\n",
        "1: automobile\n",
        "2: bird\n",
        "3: cat\n",
        "4: deer\n",
        "5: dog\n",
        "6: frog\n",
        "7: horse\n",
        "8: ship\n",
        "9: truck"
      ],
      "metadata": {
        "id": "fdJIWJhu8OTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imorting dependencies\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "FsOWmk01dvT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOpzj8XrdoP9"
      },
      "outputs": [],
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyper Paramteres\n",
        "num_epoch = 4\n",
        "batch_size = 4\n",
        "learning_rate = 0.001"
      ],
      "metadata": {
        "id": "11emQpXJePt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset has PILImage images of range [0,1]\n",
        "#we transform them to sensors and normalise between [-1,1]\n",
        "# Normalize(mean, std[, inplace]) : Normalize a tensor image with mean and standard deviation.\n",
        "\n",
        "transform = transforms.Compose(\n",
        "                                  [transforms.ToTensor(),\n",
        "                                  transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))]\n",
        "                              )\n"
      ],
      "metadata": {
        "id": "RUBTBokFea7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#downloading and splitting the dataset\n",
        "# Importing the dataset from CIFAR and transforming\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data',train=True,\n",
        "                                           download=True, transform = transform)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data',train=False,\n",
        "                                           download = True, transform = transform)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pl-S3llgh0kY",
        "outputId": "e7992801-e2a4-4a88-aa27-13c025251c3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:04<00:00, 42297324.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Loaders\n",
        "train_loader = DataLoader(dataset= train_dataset, batch_size = batch_size,shuffle = True)\n",
        "test_loader = DataLoader(dataset= test_dataset, batch_size = batch_size,shuffle = False)"
      ],
      "metadata": {
        "id": "iCVFRffIiVMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defining the classes\n",
        "classes = ('plane','car','bird','cat','deer','dog','frog','horse','ship','truck')"
      ],
      "metadata": {
        "id": "lp976RV6i8kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IMPLEMENTING CONV NET"
      ],
      "metadata": {
        "id": "eFFhxW67jMVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvNet(nn.Module):\n",
        "\n",
        "  #creating conv layer and fully connected layer\n",
        "  def __init__(self):\n",
        "    super(ConvNet, self).__init__()\n",
        "    #creating 1st conv layer,input has 3 channels, 6 - output as 6 channels, kernal or filter to slide over image -5 \n",
        "    self.conv1 = nn.Conv2d(3,6,5)\n",
        "    #max pooling layer 2x2\n",
        "    self.pool = nn.MaxPool2d(2,2)\n",
        "    #creating 2nd conv layer, input channel must equal to last output channel\n",
        "    self.conv2 = nn.Conv2d(6,16,5)\n",
        "\n",
        "    #fully connected 3 layers with input size and output size\n",
        "    #16x5x5 - got output of 2nd conv layer as 16 as output channel and 5x5 after pooling\n",
        "    self.fc1 = nn.Linear(16*5*5, 120)\n",
        "    self.fc2 = nn.Linear(120, 84)\n",
        "    self.fc3 = nn.Linear(84, 10) # 10 for 10 different classes\n",
        "\n",
        "  #defining forward, x will get the sample as input\n",
        "  def forward(self,x):\n",
        "    #conv1 --> relu --> pooling --> conv2 -- relu --> pooling --Flattening --> fully con layer\n",
        "    x = self.pool(F.relu(self.conv1(x)))\n",
        "    x = self.pool(F.relu(self.conv2(x)))\n",
        "    #flatening\n",
        "    x= x.view(-1,16*5*5)\n",
        "    #passing to fullycon layer1 --> applying relu function --> fullycon2 --> relu --> fullycon3 -- > output\n",
        "    x= F.relu(self.fc1(x))\n",
        "    x= F.relu(self.fc2(x))\n",
        "    x= self.fc3(x)\n",
        "    return x\n",
        "\n",
        "model = ConvNet().to(device)"
      ],
      "metadata": {
        "id": "Qm6HKktqjK0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LB70rLTZwHfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LOSS AND OPTIMISER"
      ],
      "metadata": {
        "id": "PRJYKm9T2cK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "7P4b9_6k2dt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRAINING LOOP"
      ],
      "metadata": {
        "id": "N7AikrQM2qz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_total_steps = len(train_loader)\n",
        "for epoch in range(num_epoch):\n",
        "    for i, (images, labels) in enumerate(train_loader):  #i - index\n",
        "        # origin shape: [4, 3, 32, 32] = 4, 3, 1024\n",
        "        # input_layer: 3 input channels, 6 output channels, 5 kernel size\n",
        "        images = images.to(device) #pushing to GPU for GPU support\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 2000 == 0:\n",
        "            print (f'Epoch [{epoch+1}/{num_epoch}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
        "\n",
        "print('Finished Training')\n",
        "PATH = './cnn.pth'\n",
        "torch.save(model.state_dict(), PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wiufQAj72sOK",
        "outputId": "9a290b02-cbc7-4ae5-d622-c2f42455dc8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/4], Step [2000/12500], Loss: 1.6754\n",
            "Epoch [1/4], Step [4000/12500], Loss: 2.3011\n",
            "Epoch [1/4], Step [6000/12500], Loss: 2.1244\n",
            "Epoch [1/4], Step [8000/12500], Loss: 1.0895\n",
            "Epoch [1/4], Step [10000/12500], Loss: 1.1142\n",
            "Epoch [1/4], Step [12000/12500], Loss: 1.2080\n",
            "Epoch [2/4], Step [2000/12500], Loss: 1.3731\n",
            "Epoch [2/4], Step [4000/12500], Loss: 1.5377\n",
            "Epoch [2/4], Step [6000/12500], Loss: 1.6718\n",
            "Epoch [2/4], Step [8000/12500], Loss: 1.2515\n",
            "Epoch [2/4], Step [10000/12500], Loss: 0.8482\n",
            "Epoch [2/4], Step [12000/12500], Loss: 1.8774\n",
            "Epoch [3/4], Step [2000/12500], Loss: 1.7664\n",
            "Epoch [3/4], Step [4000/12500], Loss: 2.6744\n",
            "Epoch [3/4], Step [6000/12500], Loss: 2.2780\n",
            "Epoch [3/4], Step [8000/12500], Loss: 2.2778\n",
            "Epoch [3/4], Step [10000/12500], Loss: 0.7260\n",
            "Epoch [3/4], Step [12000/12500], Loss: 0.9087\n",
            "Epoch [4/4], Step [2000/12500], Loss: 1.0786\n",
            "Epoch [4/4], Step [4000/12500], Loss: 1.1327\n",
            "Epoch [4/4], Step [6000/12500], Loss: 1.8083\n",
            "Epoch [4/4], Step [8000/12500], Loss: 1.3081\n",
            "Epoch [4/4], Step [10000/12500], Loss: 1.8284\n",
            "Epoch [4/4], Step [12000/12500], Loss: 1.1060\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TESTING "
      ],
      "metadata": {
        "id": "3dXejrb85pF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    n_class_correct = [0 for i in range(10)]\n",
        "    n_class_samples = [0 for i in range(10)]\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        # max returns (value ,index)\n",
        "        _, predicted = torch.max(outputs, 1) #1 for index 1\n",
        "        n_samples += labels.size(0)\n",
        "        n_correct += (predicted == labels).sum().item()\n",
        "        \n",
        "        for i in range(batch_size):\n",
        "            label = labels[i]\n",
        "            pred = predicted[i]\n",
        "            if (label == pred):\n",
        "                n_class_correct[label] += 1\n",
        "            n_class_samples[label] += 1\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the network: {acc} %')\n",
        "\n",
        "    for i in range(10):\n",
        "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
        "        print(f'Accuracy of {classes[i]}: {acc} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyFsYPSg5rQh",
        "outputId": "ebd1ee6a-32a5-46b4-80fb-a299a62a7e3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network: 54.44 %\n",
            "Accuracy of plane: 56.4 %\n",
            "Accuracy of car: 75.0 %\n",
            "Accuracy of bird: 33.6 %\n",
            "Accuracy of cat: 23.8 %\n",
            "Accuracy of deer: 44.9 %\n",
            "Accuracy of dog: 52.9 %\n",
            "Accuracy of frog: 68.1 %\n",
            "Accuracy of horse: 64.8 %\n",
            "Accuracy of ship: 68.2 %\n",
            "Accuracy of truck: 56.7 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p0P5HXjJ56o8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}